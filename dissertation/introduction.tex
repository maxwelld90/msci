%!TEX root = sigir2013site_clicks.tex
\section{Introduction}\label{sec_introduction}
Today, it is uncommon to find an organisation without a presence on the \emph{World Wide Web (Web)}. With many organisations hosting thousands of pages of content, site administrators typically deploy some sort of search functionality to help users find relevant information. Restricted to the content hosted by the organisations in question, such search engine functionality can be defined as \emph{site search}.

Users of site search systems differ depending on the site that is being visited. Users of a governmental website for example may be interested in finding information related to tax returns and driving licences. Users of an academic institution's website may wish to find information about degree programmes the institution is offering. Studies of user behaviour on site search engines have shown that users issue queries which are constrained to the topics encompassed by the organisation in question, with many similar queries are repeated \cite{chau2005site_log_analysis}. This repetitive nature therefore allows us to gather implicit feedback and collate it together to improve performance.

In contrast to site search, \citeauthor{chau2005site_log_analysis} \cite{chau2005site_log_analysis} defines \emph{Web search} as a general-purpose search engine, unrestricted to a particular domain or specialty. Such search engines - such as \emph{Google}\footnote{\url{http://www.google.com}} and \emph{Bing}\footnote{\url{http://www.bing.com}} - each have a wide-ranging index, consisting of billions of documents. This large index may present challenges keeping the index comprehensive and up-to-date, which could potentially lead to results with low precision and recall. \citeauthor{henzinger2002challenges_in_web_search} \cite{henzinger2002challenges_in_web_search} presents a series of further challenges faced by Web search engines. These include dealing with high volumes of spam, wildly varying levels of content quality, and the disregarding of particular `web conventions' by administrators of different sites. Despite Web search engines attempting to cater for a very large and diverse population of users, such search engines generally perform better than site search - a seemingly more constrained problem.

This issue can be attributed to the fact that site search systems have inherent operational challenges which reduce their retrieval performance \cite{ding2007log_based_site_search}. Organisations typically do not have the resources available to investigate means of improving their site search deployment. The organisation may also lack the technical skills to interpret usage data of their search engine. This is compounded by the fact that site search will generate a smaller volume of usage data when contrasted to Web search engines. The smaller volume of usage data could potentially reduce retrieval effectiveness if the data were used to evaluate or tune the search engine.

In its current guise, site search is therefore typically inadequate for a majority of websites that employ it \cite{ding2007log_based_site_search, xue2002log_mining}. It has been argued that search logs could be used to improve site search performance \cite{guo2009click_chain, joachims2002optimizing_clickthrough}. In particular, clickthrough data - which has been successfully leveraged by Web search engines - could be extracted from these logs, and are regarded as a good feature on determining how relevant documents are in relation to a given query \cite{joachims2005clickthrough}.

In this paper, we propose three simple models that combine readily-available clickthrough data with a baseline ranking function with the aim of improving site search performance. We then investigate the behaviour of clickthrough data by performing a series of simulations. These simulations vary the levels of noise and biases which real-world clickthrough data exhibits. We then discuss our findings in detail.

The remainder of this paper is organised as follows. Section \ref{sec_background} reviews related works in the areas of improving the performance of site search, and how clickthrough data has been used. Section \ref{sec_method} presents our research questions, our devised clickthrough models and an overview of our simulations. Section \ref{sec_results} presents our results, while section \ref{sec_conclusion} discusses and concludes the work.
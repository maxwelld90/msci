%!TEX root = sigir2013site_clicks.tex
\section{Discussion and Conclusion}\label{sec_conclusion}
From our simulations discussed in Section \ref{sec_results}, we found several interesting features regarding the influences of clickthrough data on document rankings. We also observed several behavioural changes in clickthrough data when aspects such as noise and bias were introduced and adjusted. These observations allowed us to ascertain an understanding of the behaviour of clickthrough data, so often concealed within the `black box' of a machine-learned approach typically used in today's research.

\subsection{Summary of Findings}\label{sec:conclusion:summary}
Findings from our clickthrough simulations demonstrate a complex space, with many possible combinations and outcomes. Here, we briefly summarise our key findings regarding the behaviour of clickthrough data, and discuss the performance benefits observed.

We have ascertained that incorporating clickthrough data into search rankings has the potential to greatly improve performance. 
However, as a form of implicit relevance feedback, the performance gains that clickthrough data could potentially provide are severely limited through aspects such as noise (clicks on irrelevant documents) and bias (such as positional bias). To demonstrate the performance gains and losses observed, Table \ref{tbl:conclusion:summary} highlights performance figures obtained across all simulations run.

Under ideal conditions free from noise and bias, clickthrough data has the potential to make significant performance improvements over our baseline values. Indeed, under ideal conditions, model \textbf{PD} attained a MAP of 0.6269 over our baseline MAP of 0.1842. A P@10 value of 0.806 indicated that 8 out of 10 documents in the top 10 rankings were relevant - a huge improvement. As we began to factor in attributes such as noise and bias, we began to observe a drop in performance - as expected. However, our findings show that noise and bias can theoretically be overcome.

We showed that setting the probability of irrelevant clicks (noise, or $P(C|N)$) high enough will ensure that performance never improves upon baseline values. However, a more realistic value (between 0.25 and 0.5) was observed to allow for performance improvements similar - if not identical to - our `perfect world' simulations. However, as shown in Table \ref{tbl:conclusion:summary}, these improvements came after a much larger number of iterations. This therefore shows that irrelevant clicks can theoretically be overcome - more data is simply required to mitigate the effects that noise brings.

However, it is when positional bias was introduced that we observed perhaps the most significant drops in performance. Compared with a noise-only MAP of 0.6724, our recorded depth-proportional MAP was 0.5417. Depth-proportional or positional bias lowers the probability of a document being inspected the lower it is in the rankings. As such, documents at rank 30 have a 0.03\% chance of being examined. With an almost 0\% chance of being inspected, let alone clicked, positional bias severely affects the highest performance that can be attained. It can be said that further performance improvements could be attained over a greater number of iterations. However, such a value would be so high that there would be no advantage to site search engines.

Using parameters obtained from a real-world user study \cite{smucker2012time_based_calibration}, we still observed encouraging results. While MAP still improved from our baseline, it was significantly down on performance that was attained under ideal conditions. However, P@5 was doubled, with P@10 not far behind - showing that relevant documents were still percolating to the top of the rankings. It was clear that the probabilities used (especially of that for $P(C|N)$) hindered performance of our two more simplistic models, \textbf{P} and \textbf{PD}. \textbf{PDE} was able to offer improvements, demonstrating better tolerances.

\subsection{Discussion}\label{sec:conclusion:discussion}
Our findings have shown that there are noticeable variations in performance - both positive and negative - for even slight variations of parameters across the simulations that we ran. Specifically, our promotion and demotion weightings - $\alpha$ and $\beta$ - have been shown to be incredibly sensitive towards offering performance improvements or drops. Each model has demonstrated its own strengths and limitations, and it is clear from our findings that there is no simple `one-solution-fits-all' approach towards the issue of using clickthrough data to improve site search performance. Different sites will have different content and different users. While we acknowledge that using only the DOTGOV1 collection is a limitation, we hypothesise that our findings will generalise to other document collections. This issue is discussed in more detail in Section \ref{sec:conclusion:future}.

\begin{table}
	\renewcommand{\arraystretch}{1.3}
	\begin{center}
		\begin{small}
			\begin{tabularx}{\linewidth}{W{2cm}|W{1.1125cm}|W{1.1125cm}|W{1.1125cm}|W{1.1125cm}W{0.00000001cm}}
				
			 	& \centering{{\makebox[0pt][c]{\textbf{\emph{MAP}}}}} & \centering{\textbf{\emph{P@5}}} & \centering{{\makebox[0pt][c]{\textbf{\emph{P@10}}}}} & \centering{$i$}&
				\tabularnewline
				\hline
				
				\centering{\textbf{Baseline}}
				& \centering{0.1842}
				& \centering{0.272}
				& \centering{0.226}
				& \centering{N/A}&
				\tabularnewline[1.4em]
				\hline
				
				\centering{\textbf{Ideal}\\{\tiny\textbf{PD (0.75, 2.0)}}}
				& \centering{0.6727}
				& \centering{0.892}
				& \centering{0.81}
				& \centering{311}&
				\tabularnewline[1.4em]
				\hline
				
				\centering{\textbf{Depth}\\{\tiny\textbf{PD (0.75, 0.05)}}}
				& \centering{0.5417}
				& \centering{0.892}
				& \centering{0.778}
				& \centering{978}&
				\tabularnewline[1.4em]
				\hline
				
				\centering{\textbf{Noise}\\{\tiny\textbf{PD (0.75, 2.0)}}}
				& \centering{0.6724}
				& \centering{0.892}
				& \centering{0.81}
				& \centering{447}&
				\tabularnewline[1.4em]
				\hline
				
				\centering{\textbf{Noise/Depth}\\{\tiny\textbf{PD (0.75, 0.05)}}}
				& \centering{0.467}
				& \centering{0.836}
				& \centering{0.708}
				& \centering{994}&
				\tabularnewline[1.4em]
				\hline
				
				\centering{\textbf{Real-World}\\{\tiny\textbf{PDE (2.0, 2.0)}}}
				& \centering{0.273}
				& \centering{0.58}
				& \centering{0.408}
				& \centering{1935}&
				\tabularnewline[1.4em]
				
			\end{tabularx}
		\end{small}
	\end{center}
	\vspace{-0.3cm}
	\caption{\textbf{Table summarising the best performance observed across all simulations run. The model offering performance with the highlighted values - and associated $\alpha$ and $\beta$ values (in brackets) - are shown underneath each simulation's title.}\vspace{-0.5cm}}
	\label{tbl:conclusion:summary}
\end{table}

One of the disadvantages of clickthrough data is noise. Noise has been observed to degrade performance in the number of iterations required for a significant improvement to be observed. However, with the probability of a click on an irrelevant document set to a high value ($>=0.5$), no improvement whatsoever is observed. This is described as the performance `cliff' in Section \ref{sec:results:noise}. Therefore to yield good results, clickthrough data can contain noise; excessive amounts however will render the data as good as useless.

With the impact of noise, we also noticed a behavioural aspect of clickthrough data related to our demotion weighting parameter, $\beta$. With our `perfect world' simulations, we observed that performance improvements were only forthcoming with a small $\beta$ value - Figure \ref{fig:perfect_map_fall} illustrates this finding. However, by varying our relevant and irrelevant probabilities, $P(C|R)$ and $P(C|N)$, we discovered an intuitive link between $\beta$ and $P(C|R)$. The higher the $P(C|R)$, the more confident one could assume clicks were on relevant documents, hence a $\beta$ value would allow for higher performance to be attained in fewer iterations. Moreover, a lower $P(C|R)$ removed this confidence, and a smaller $\beta$ - such as those demonstrated to offer good improvements in Figure \ref{fig:perfect_map_fall} - would suffice. 

Bias - specifically positional bias - directly affects the number of iterations required for performance improvements to be observed, as discussed in Section \ref{sec:conclusion:summary}. This has a direct effect on site search due to the smaller number of users these search engines may experience compared to, for example, a commercial Web search engine. We found again that a slight variation in $\beta$ values could mitigate the effects of positional bias. A slightly higher $\beta$ value would give the demotion aspect of each model a greater chance to drop irrelevant documents down rankings.

There are many variations and permutations that can be selected. Not all are suited to site search scenarios. There are however several trade-offs that can be made. Each models offers its own advantages and disadvantages depending on the situation. For example, an environment with high quality data would be ideally suited to model \textbf{P}, and performance could theoretically be reached in a short time frame. Where precision is key, \textbf{PD} may be suited - but is not too robust. \textbf{PDE} has demonstrated an ability to provide an above-baseline improvement in performance, while tolerating higher levels of noise than the other two models. For any reasonable performance improvement, large volumes of clickthrough data are necessary due to the imperfections this form of data brings.

\subsection{Future Work}\label{sec:conclusion:future}
The research undertaken as part of this study has raised several interesting points that we feel warrant future work. Here, we address these points, discussing why undertaking them would be beneficial to the wider IR community.

Our first point regards the use of test collections. As previously discussed in Sections \ref{sec:method:materials} and above in Section \ref{sec:conclusion:discussion}, we ran our simulations with the TREC DOTGOV1 corpus only. This was regarded as a sensible decision due to the large volumes of data generated and limited space to report results. However, it would nevertheless be interesting to determine whether our findings hold for different collections of documents, or if adjustments must be made depending on factors such as the corpus size or genre. This issue raised also links into the idea of utilising a \emph{true} website's document collection in place TREC collections.

Secondly, we placed a cap on the maximum number of iterations that each of our simulations could run. Unfortunately, many of these simulations reached the maximum iteration count (see Tables \ref{tbl:results_perfect_view} and \ref{tbl:results_biased_demotion} for examples). This suggests that a greater level of performance could have been attained than we recorded, perhaps after several thousand iterations per simulation. Future work could remove such a cap and allow the simulations to reveal their true attainable maximum performance. This would allow for a better comparison between simulation results, and provide a better understanding of the volumes of data required to reach maximum performance.

With performance of our realistic simulations somewhat disappointing compared to our `perfect world' results, future work could investigate a series of models which are even more robust to factors of real-world clickthrough data, such as noise and different forms of bias. Finally, a form of machine-learned ranking approach would be beneficial as it would provide us with a meaningful set of results to compare our findings against.

Our findings have contributed to our understanding of clickthrough data, and what effects bias and noise have on such data. Results have provided an insight into what takes place inside a `black-box', machine-learned approach to optimising search engines when incorporating clickthrough data. Further investigation would allow us to improve our understanding of clickthrough data and how it can be used to improve search engine performance.

%\todo{=========LEIF'S COMMENTS========}
%remove p graphs, put red line in the other four and say that this is P's performance.
%just have 0.0, 0.1, 0.25, 2.0. take out stuff in the middle, makes it look cluttered.

%perhaps readjust size of graphs, make them as big as possible.

%in the tables, try remove the redundancy - i.e. remove the PD, PDE stuff...
%a row for PD, a row for PDE so it removes the PD/PDE duplications...

%can performance of 0.1840 with i = 1000 be shortened to something like 1.84 (1000)???